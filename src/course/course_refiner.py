# === í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ===
import os, json, time
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = Path(__file__).resolve().parents[2]
ENV_PATH = BASE_DIR / ".env"
COURSE_DIR = BASE_DIR / "data" / "course_db"
FILTER_DIR = COURSE_DIR / "filtered"
FILTER_DIR.mkdir(parents=True, exist_ok=True)

# === í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ===
load_dotenv(ENV_PATH, override=True)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# === ì„ë² ë”© ëª¨ë¸ (í•œë²ˆë§Œ ë¡œë“œ) ===
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")

# === ì½”ìŠ¤ ê²€ìˆ˜ ===
PROMPT_FILTER = """
ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ë°˜ í•™ìŠµ ì½”ìŠ¤ì˜ ê¸°íš ê²€ìˆ˜ìì´ë‹¤.
ë‹¤ìŒì€ ì—¬ëŸ¬ ê°œì˜ 'ì½”ìŠ¤ ì´ë¦„' ëª©ë¡ì´ë‹¤.
ê° ì½”ìŠ¤ê°€ í•™ìŠµ ì£¼ì œë¡œì„œ íƒ€ë‹¹í•œì§€ íŒë‹¨í•˜ë¼.

---

### íŒë‹¨ ê¸°ì¤€

#### **í•™ìŠµì  ê°€ì¹˜ê°€ ìˆëŠ” ì½”ìŠ¤(True)**
ë‹¤ìŒ ì¤‘ í•˜ë‚˜ ì´ìƒì— í•´ë‹¹í•˜ë©´ Trueë¡œ íŒë‹¨í•œë‹¤.

1. **ì‚¬íšŒ êµ¬ì¡°Â·ì •ì±… ë§¥ë½** â€” ì œë„ë‚˜ ì •ì±…ì˜ ë°©í–¥ì„±, ì‚¬íšŒ êµ¬ì¡°ì  ë³€í™”ì™€ ê´€ë ¨ëœ ì£¼ì œ  
2. **ì¸ê³¼ê´€ê³„ì™€ ë³€í™” íë¦„** â€” ê¸°ìˆ Â·ê²½ì œ ë³€í™”ê°€ ì‚¬íšŒ ì „ë°˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥  
3. **ì„¸ëŒ€Â·ê°€ì¹˜ê´€Â·ì‚¬íšŒ ì¸ì‹** â€” ì„¸ëŒ€ ê°„ ì¸ì‹ ì°¨ì´, ì‚¬íšŒë¬¸í™”ì  ë³€í™”ë¥¼ ë‹¤ë£¨ëŠ” ì£¼ì œ  
4. **ë…¼ìŸê³¼ ê· í˜• ì‹œê°** â€” ì°¬ë°˜ êµ¬ì¡°ë‚˜ ì‚¬íšŒì  ê°ˆë“±ì„ ê· í˜• ìˆê²Œ íƒêµ¬í•˜ëŠ” ì£¼ì œ  
5. **êµ­ì œ ê´€ê³„Â·ê²½ì œ êµ¬ì¡°** â€” ì„¸ê³„ ê²½ì œ, ì™¸êµ ê´€ê³„, êµ­ì œ í˜‘ë ¥ êµ¬ì¡°ë¥¼ ë‹¤ë£¨ëŠ” ì£¼ì œ  
6. **ì‚¬íšŒ ë¬¸ì œÂ·ì œë„ ë³€í™”** â€” ë³µì§€, êµìœ¡, ë…¸ë™, ì£¼ê±° ë“± ì‚¬íšŒ ì‹œìŠ¤í…œ ë³€í™”ì™€ ì—°ê´€ëœ ì£¼ì œ  
7. **ê¸°ìˆ ê³¼ ì‚¬íšŒì˜ ì ‘ì ** â€” ê¸°ìˆ  ë°œì „ì´ ì¸ê°„ ì‚¶, êµìœ¡, ì‚¬íšŒ êµ¬ì¡°ì— ë¯¸ì¹˜ëŠ” ì˜ë¯¸  
8. **ê³µê³µì„±Â·ìœ¤ë¦¬Â·ì±…ì„** â€” ìœ¤ë¦¬, ì •ì±…, ESG, ì‚¬íšŒì  ì±…ì„ ë“± ê³µê³µ ê°€ì¹˜ ë…¼ì˜  
9. **ë§¥ë½ì  ì„œì‚¬(ë³€í™”ì˜ ë°°ê²½)** â€” ì‚¬íšŒ ë³€í™”ì˜ ì›ì¸ê³¼ ê²°ê³¼ë¥¼ ì—°ê²°í•˜ëŠ” ì„œì‚¬ êµ¬ì¡°  
10. **ê°ì •Â·ê°€ì‹­ ì¤‘ì‹¬ ì—¬ë¶€(ë¶€ì •)** â€” ìê·¹ì  ê°ì •ì´ë‚˜ ê°œì¸ ë¹„ë‚œë³´ë‹¤ ì‚¬íšŒì  ì„±ì°°ì„ ìœ ë„í•˜ëŠ” ì£¼ì œ

---

#### **í•™ìŠµì  ê°€ì¹˜ê°€ ë‚®ì€ ì½”ìŠ¤(False)**
ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ë‘ë“œëŸ¬ì§€ë©´ Falseë¡œ íŒë‹¨í•œë‹¤.

- ì¸ë¬¼ ì¤‘ì‹¬ì˜ ë‹¨ìˆœ ë°œì–¸Â·ì†Œê°Â·ë…¼ë€ ì¤‘ì‹¬  
- ì‚¬ê±´Â·ì‚¬ê³ , ë²”ì£„, ì¬ë‚œ, í™”ì¬, êµí†µ, ë‚ ì”¨ ë“± ë‹¨ë°œì„± ë³´ë„  
- ê¸°ì—…Â·ë¸Œëœë“œ í™ë³´, ì‹ ì œí’ˆ ì¶œì‹œ, í–‰ì‚¬Â·ì¶•ì œÂ·ì‹œìƒì‹ ë“± ìƒì—…ì„± ê¸°ì‚¬  
- ê°ì •ì  ëŒ€ë¦½, ë¹„ë‚œ, ì„ ì •ì  ì œëª© ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì¡°Â·ì •ì±… ë§¥ë½ì´ ì•½í•œ ê²½ìš°  
- ë‹¨ìˆœ ì •ì¹˜ ê³µë°©, ì—¬ì•¼ ë¹„ë‚œ êµ¬ë„ë§Œ ë°˜ë³µë˜ëŠ” ê²½ìš°  

---


ì¶œë ¥ í˜•ì‹(JSON Lines):
[
  {"courseName": "ì½”ìŠ¤ëª…", "is_educational": true or false, "reason": "íŒë‹¨ ê·¼ê±°"}
]

ì£¼ì˜:
1. ì ˆëŒ€ë¡œ ```json ê°™ì€ ë§ˆí¬ë‹¤ìš´ í‘œì‹œëŠ” ì“°ì§€ ë§ê³ ,
2. ìˆœìˆ˜ JSONë§Œ ë°˜í™˜í•˜ë¼.
"""

# === ì•ˆì „í•œ LLM ìš”ì²­ í•¨ìˆ˜ ===
def safe_request(prompt, retry=3, wait=3):
    for attempt in range(retry):
        try:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ë‰´ìŠ¤ ì½”ìŠ¤ì˜ í’ˆì§ˆ í‰ê°€ìë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
            )
            return resp.choices[0].message.content
        except Exception as e:
            print(f"[ê²½ê³ ] ì¬ì‹œë„ {attempt+1}/{retry}: {e}")
            time.sleep(wait)
    return None


# === ì½”ìŠ¤ëª… ê¸°ë°˜ í•„í„°ë§ ===
def filter_by_course_name(courses, batch_size=8):
    results = []
    for i in tqdm(range(0, len(courses), batch_size), desc="1ë‹¨ê³„: ì½”ìŠ¤ëª… LLM í•„í„°ë§"):
        batch = courses[i:i+batch_size]
        names = "\n".join([f"- {c.get('courseName', '')}" for c in batch])
        prompt = f"{PROMPT_FILTER}\n\n{names}"

        resp = safe_request(prompt)
        if not resp:
            for c in batch:
                results.append({
                    "courseName": c.get("courseName", ""),
                    "is_educational": True,
                    "reason": "API ì‘ë‹µ ì‹¤íŒ¨ (ì„ì‹œ í†µê³¼)"
                })
            continue

        try:
            parsed = json.loads(resp)
            if isinstance(parsed, list):
                results.extend(parsed)
            else:
                results.append(parsed)
        except Exception as e:
            print(f"[íŒŒì‹± ì˜¤ë¥˜] {e}\nì‘ë‹µ:\n{resp[:200]}...\n")
            for c in batch:
                results.append({
                    "courseName": c.get("courseName", ""),
                    "is_educational": True,
                    "reason": "JSON íŒŒì‹± ì‹¤íŒ¨ (ì„ì‹œ í†µê³¼)"
                })
    return results

def compute_coherence(course, noise_threshold=0.4, return_removed=False):
    """
    í•œ ì½”ìŠ¤ ë‚´ headline ì„ë² ë”© ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ ,
    ì¤‘ì‹¬ ë²¡í„°ì™€ì˜ cosine similarityê°€ ë‚®ì€ ì„¸ì…˜ì„ ì œê±°í•œë‹¤.
    return_removed=Trueì¼ ê²½ìš° ì œê±°ëœ headline ëª©ë¡ë„ ë°˜í™˜í•œë‹¤.
    """
    headlines = [s.get("headline", "") for s in course.get("sessions", []) if s.get("headline")]
    if len(headlines) < 2:
        return 0.0 if not return_removed else (0.0, [])

    # === headline ì„ë² ë”© ===
    embs = embedding_model.encode(headlines, convert_to_numpy=True, normalize_embeddings=True)

    # === ì„¸ì…˜ ê°„ í‰ê·  ìœ ì‚¬ë„ ===
    sims_matrix = cosine_similarity(embs)
    upper = np.triu_indices_from(sims_matrix, k=1)
    coherence_score = float(np.mean(sims_matrix[upper]))

    # === ì¤‘ì‹¬ ë²¡í„°(centroid) ê³„ì‚° ===
    centroid = np.mean(embs, axis=0)
    centroid_sims = cosine_similarity(embs, centroid.reshape(1, -1)).flatten()

    # === ë…¸ì´ì¦ˆ ì„¸ì…˜ í•„í„°ë§ ===
    kept_sessions = []
    removed_sessions = []
    for i, s in enumerate(course["sessions"]):
        sim_score = float(centroid_sims[i])
        s["similarity_to_center"] = sim_score
        if sim_score >= noise_threshold:
            kept_sessions.append(s)
        else:
            removed_sessions.append({
                "headline": s.get("headline", ""),
                "similarity": sim_score
            })

    # === ì½”ìŠ¤ ë‚´ ì„¸ì…˜ ì—…ë°ì´íŠ¸ ===
    course["sessions"] = kept_sessions

    if return_removed:
        return coherence_score, removed_sessions
    else:
        return coherence_score
    
PROMPT_SESSION_REVIEW = """
ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ë°˜ í•™ìŠµ ì½”ìŠ¤ì˜ ê²€ìˆ˜ìì´ë‹¤.
ë‹¤ìŒì€ í•œ ì½”ìŠ¤ì˜ ì„¸ì…˜(ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©) ëª©ë¡ì´ë‹¤.
**ì½”ìŠ¤ì˜ ì£¼ì œ íë¦„ì„ ìœ ì§€í•˜ë©´ì„œ**, í•´ë‹¹ ì£¼ì œì™€ ê´€ë ¨ì„±ì´ ë‚®ê±°ë‚˜ ì™„ì „íˆ ë²—ì–´ë‚œ ì„¸ì…˜ë§Œ ê³¨ë¼ë¼.

---

íŒë‹¨ ê¸°ì¤€:

1ï¸. **ìœ ì§€(True)**  
    - ì½”ìŠ¤ì˜ ì£¼ì œ(ì½”ìŠ¤ëª…)ì™€ ì§ì ‘ì  í˜¹ì€ êµ¬ì¡°ì ìœ¼ë¡œ ì—°ê²°ëœ ê¸°ì‚¬  
    - ì •ì±…, ì œë„, ê²½ì œÂ·ì‚¬íšŒ êµ¬ì¡°, ì™¸êµÂ·ì‚°ì—… ë³€í™” ë“± ì‹œì‚¬ì  ë§¥ë½ì´ ìˆëŠ” ê¸°ì‚¬  
    - ì¸ë¬¼ ì¤‘ì‹¬ì´ë¼ë„ ì½”ìŠ¤ ì£¼ì œì™€ ì—°ê´€ëœ ë°œì–¸, ì •ì±… ë…¼ì˜, ì‚¬íšŒì  ì˜ë¯¸ê°€ ìˆìœ¼ë©´ ìœ ì§€  

2ï¸. **ì œê±°(False)**  
    - ë‹¤ìŒ í•­ëª©ì— í•˜ë‚˜ë¼ë„ í•´ë‹¹ë˜ë©´ ë°˜ë“œì‹œ ì œê±°í•œë‹¤.
    [ì½”ìŠ¤ ì£¼ì œì™€ ë¬´ê´€]
    - ì½”ìŠ¤ ì£¼ì œì™€ ì§ì ‘ ê´€ë ¨ì´ ì—†ìŒ 
    - ë™ì¼í•œ ì£¼ì œÂ·ë‚´ìš©ì„ ë°˜ë³µí•˜ëŠ” ê²½ìš° ëŒ€í‘œ 1ê°œë§Œ ìœ ì§€
    [í•™ìŠµìš© ë¶€ì í•©]
    - ê¸°ì—…Â·ì œí’ˆÂ·ì„œë¹„ìŠ¤Â·ë¸Œëœë“œ í™ë³´ì„± ê¸°ì‚¬
    - ì‹ ì œí’ˆ ì¶œì‹œ, í”„ë¡œëª¨ì…˜, ë§ˆì¼€íŒ… ìº í˜ì¸, ì¸í„°ë·° ì¤‘ì‹¬ ê¸°ì‚¬ í¬í•¨
    - ì¼íšŒì„± ì´ë²¤íŠ¸/í–‰ì‚¬ ì¤‘ì‹¬ ê¸°ì‚¬
    - ì§€ì—­ ì¶•ì œ, ì‹œìƒì‹, ì½˜ì„œíŠ¸, ì „ì‹œíšŒ, ë‹¨ì²´ í–‰ì‚¬ ë“±
    - ë‹¨ìˆœ ì •ë³´Â·í†µê³„Â·ì°¸ì„ì ìˆ˜ ì¤‘ì‹¬ ê¸°ì‚¬
    - ìˆ˜ìƒ ì¸ì›, ì°¸ì—¬ ì¸ì›, ìˆœìœ„ ë°œí‘œ ë“± ë§¥ë½ ì—†ëŠ” ìˆ«ì ë‚˜ì—´í˜• ê¸°ì‚¬
    - ì •ì±…Â·ì‚¬íšŒì  ì˜ë¯¸ ì—†ì´ ì¸ë¬¼Â·ë°œì–¸ë§Œ ì „ë‹¬í•˜ëŠ” ê¸°ì‚¬
    - ì—°ì˜ˆÂ·ìŠ¤í¬ì¸ Â·ë‚ ì”¨Â·ì¬ë‚œ ë“± ë¹„í•™ìŠµì„± ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬
    - ì‚¬íšŒ êµ¬ì¡°ë‚˜ ì •ì±…ì  ì‹œì‚¬ì ì´ ì—†ëŠ” ê²½ìš° ëª¨ë‘ ì œê±°
    - ìœ ì‚¬ ë‚´ìš©ì˜ ë°˜ë³µ ê¸°ì‚¬ ì¤‘ ì¤‘ë³µì„± ë†’ìŒ
---

ì°¸ê³  ì˜ˆì‹œ (í•™ìŠµìš© ë¶€ì í•©):

- ì •ì¹˜: â—‹â—‹ì˜ì›, ì§€ì—­êµ¬ ë³µì§€ì‹œì„¤ ë°©ë¬¸â€¦ì£¼ë¯¼ë“¤ê³¼ ê°„ë‹´íšŒ  
  â†’ í‘œë©´ìƒ ë³µì§€ì •ì±… ê´€ë ¨ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ê°œì¸ í™œë™ ì¤‘ì‹¬ ê¸°ì‚¬ë¡œ ì œë„Â·ì •ì±… ë³€í™” ë§¥ë½ì´ ì—†ìŒ  

- ê²½ì œ: ëŒ€ê¸°ì—… â—‹â—‹, í˜‘ë ¥ì‚¬ ê°„ë‹´íšŒ ì—´ê³  ìƒìƒí˜‘ë ¥ ê°•ì¡°  
  â†’ â€˜ìƒìƒâ€™ì´ë¼ëŠ” ë‹¨ì–´ê°€ ì •ì±…ì²˜ëŸ¼ ë³´ì´ë‚˜, ê¸°ì—… í™ë³´ì„± ê¸°ì‚¬ë¡œ êµ¬ì¡°ì  ê²½ì œ ë…¼ì˜ ì—†ìŒ  

- ì‚¬íšŒ: ì„œìš¸ì‹œ, ë²šê½ƒì¶•ì œ ê°œë§‰â€¦ì‹œë¯¼ 50ë§Œ ìš´ì§‘  
  â†’ í–‰ì •ì´ ì–¸ê¸‰ë˜ì§€ë§Œ ì •ì±… ë³€í™” ì—†ì´ ë‹¨ìˆœ í–‰ì‚¬ ì¤‘ì‹¬ ê¸°ì‚¬  

- êµ­ì œ: í•œì¼ ì •ìƒ, ì§§ì€ í™˜ë‹´â€¦ìš°í˜¸ ë¶„ìœ„ê¸° ì¡°ì„±  
  â†’ ì™¸êµ ì£¼ì œ ê°™ì§€ë§Œ, í˜‘ì •Â·í•©ì˜ ë“± ì‹¤ì§ˆ ë‚´ìš© ì—†ì´ ë¶„ìœ„ê¸° ë¬˜ì‚¬ ì¤‘ì‹¬ì´ë©´ í•™ìŠµ ê°€ì¹˜ ë‚®ìŒ  

- ê¸°ìˆ : â—‹â—‹ëŒ€ ì—°êµ¬íŒ€, ì‹ ê¸°ìˆ  ê°œë°œ ì„±ê³µâ€¦ìƒìš©í™” ê¸°ëŒ€  
  â†’ ê¸°ìˆ  ìì²´ëŠ” ì¤‘ìš”í•˜ë‚˜ ì‚¬íšŒÂ·ì‚°ì—… êµ¬ì¡°ì  ì˜ë¯¸ê°€ ë¶€ì¬í•œ ë‹¨ìˆœ ì„±ê³¼ ê¸°ì‚¬

---

ì¶œë ¥ í˜•ì‹(JSON):

{
  "off_topic": ["headline1", "headline2", ...],
  "reason": "ì´ ê¸°ì‚¬ë“¤ì´ ì½”ìŠ¤ ì£¼ì œì™€ ê´€ë ¨ì´ ì•½í•˜ê±°ë‚˜ ë§¥ë½ì´ ë‹¤ë¥´ë‹¤ê³  íŒë‹¨í•œ ì´ìœ ë¥¼ ê°„ë‹¨íˆ ì„¤ëª…"
}
"""

def review_sessions_with_llm(course):
    """LLMì´ ì½”ìŠ¤ ë‚´ ì„¸ì…˜ headlineì„ ê²€ìˆ˜í•˜ê³  off-topic ì„¸ì…˜ì„ ì œê±°"""
    headlines = "\n".join([f"- {s['headline']}" for s in course.get("sessions", [])])
    prompt = f"{PROMPT_SESSION_REVIEW}\n\nì½”ìŠ¤ëª…: {course['courseName']}\n\n{headlines}"

    resp = safe_request(prompt)
    if not resp:
        return course  # ì‹¤íŒ¨ ì‹œ ê·¸ëŒ€ë¡œ ë°˜í™˜

    # === ì‘ë‹µ ì „ì²˜ë¦¬ (```json ë“± ì œê±°) ===
    resp_clean = (
        resp.strip()
        .replace("```json", "")
        .replace("```", "")
        .strip()
    )

    try:
        result = json.loads(resp_clean)
        off_topics = set(result.get("off_topic", []))
    except Exception as e:
        print(f"[ì„¸ì…˜ ê²€ìˆ˜ JSON íŒŒì‹± ì˜¤ë¥˜] {e}\n{resp[:200]}...\n")
        return course

    # === ê´€ë ¨ ì—†ëŠ” ì„¸ì…˜ ì œê±° ===
    filtered_sessions = []
    removed_sessions = []
    for s in course["sessions"]:
        if s["headline"] in off_topics:
            removed_sessions.append(s["headline"])
        else:
            filtered_sessions.append(s)

    if removed_sessions:
        print(f"\n[{course['courseName']}] LLM í›„ì²˜ë¦¬ë¡œ ì œê±°ëœ ì„¸ì…˜ ({len(removed_sessions)}ê°œ):")
        for h in removed_sessions:
            print(f"  - {h}")

    course["sessions"] = filtered_sessions
    return course

# === ì¸ë±ìŠ¤ ì¬ì •ë ¬ í•¨ìˆ˜ ===
def reindex_courses(courses):
    """courseId ë° sessionId ì¬ì •ë ¬"""
    for i, course in enumerate(courses):
        course["courseId"] = i + 1  # 1ë¶€í„° ì‹œì‘
        for j, session in enumerate(course.get("sessions", [])):
            session["sessionId"] = j + 1
    return courses

# === ë©”ì¸ íŒŒì´í”„ë¼ì¸ ===
def main():
    for file in COURSE_DIR.glob("*.json"):
        print(f"\n{file.name} ì²˜ë¦¬ ì¤‘...")

        # === ë°ì´í„° ë¡œë“œ ===
        with open(file, "r", encoding="utf-8") as f:
            data = json.load(f)

        # === ğŸ”¸ì •ì¹˜ JSON íŒŒì¼ì€ í•„í„°ë§ ì—†ì´ ê·¸ëŒ€ë¡œ ë³µì‚¬ ===
        if "politics" in file.name.lower():
            out_path = FILTER_DIR / file.name
            with open(file, "r", encoding="utf-8") as src, open(out_path, "w", encoding="utf-8") as dst:
                dst.write(src.read())
            print(f"[ì •ì¹˜ íŒŒì¼] í•„í„°ë§ ê±´ë„ˆëœ€ â€” ì›ë³¸ ë³µì‚¬ ì™„ë£Œ â†’ {out_path.name}")
            continue  # ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°

        # === ë°ì´í„° ë¡œë“œ ===
        with open(file, "r", encoding="utf-8") as f:
            data = json.load(f)

        # === ì •ì¹˜ ì½”ìŠ¤ ì œì™¸ ===
        non_politics = [c for c in data if c.get("topic") != "ì •ì¹˜"]

        # === 1ï¸. LLM ì½”ìŠ¤ í•„í„°ë§ ===
        filter_results = filter_by_course_name(non_politics)
        result_map = {r["courseName"]: r for r in filter_results}

        print("\n=== LLM í•„í„°ë§ ê²°ê³¼ (ìš”ì•½) ===")
        print(json.dumps(filter_results, ensure_ascii=False, indent=2))

        # === 2ï¸. ë¶€ì ì ˆ ì½”ìŠ¤ ì œê±° ===
        filtered, removed = [], []
        for c in data:
            course_name = c["courseName"]
            result = result_map.get(course_name, {})
            if result.get("is_educational", True):
                filtered.append(c)
            else:
                removed.append({
                    "courseName": course_name,
                    "reason": result.get("reason", "ì´ìœ  ì—†ìŒ")
                })

        print(f"\ní•™ìŠµìš© {len(filtered)}ê°œ / ì œê±° {len(removed)}ê°œ")

        if removed:
            print("\n=== ì œê±°ëœ ì½”ìŠ¤ ëª©ë¡ ===")
            for r in removed:
                print(f"  - {r['courseName']}  ({r['reason']})")
            print("-" * 50)

        # === 3ï¸. ì½”ìŠ¤ ì¼ê´€ì„± + ì„¸ì…˜ ë…¸ì´ì¦ˆ ì œê±° ===
        coherence_scores = []
        print("\n=== ì½”ìŠ¤ë³„ ì„¸ì…˜ ë…¸ì´ì¦ˆ ì œê±° ë° ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° ===")

        for c in tqdm(filtered, desc="2ë‹¨ê³„: ì„¸ì…˜ ì •ì œ ë° ì ìˆ˜ ê³„ì‚°"):
            score, removed_sessions = compute_coherence(c, noise_threshold=0.55, return_removed=True)
            coherence_scores.append({"courseName": c["courseName"], "score": score})

            if removed_sessions:
                print(f"\n[{c['courseName']}] ì œê±°ëœ ì„¸ì…˜ ({len(removed_sessions)}ê°œ):")
                for r in removed_sessions:
                    print(f"  - {r['headline']} ({r['similarity']:.3f})")

        coherence_scores.sort(key=lambda x: x["score"])

        # === 4. LLM ì„¸ì…˜ í›„ì²˜ë¦¬ ===  â† ì¶”ê°€
        print("\n=== 3ë‹¨ê³„: LLM ì„¸ì…˜ ì¼ê´€ì„± ì¬ê²€ìˆ˜ ===")
        refined = []
        for c in tqdm(filtered, desc="3ë‹¨ê³„: LLM ì„¸ì…˜ ê²€ìˆ˜"):
            reviewed = review_sessions_with_llm(c)
            if reviewed.get("sessions"):  # ì„¸ì…˜ì´ ë‚¨ì•„ìˆëŠ” ê²½ìš°ë§Œ ìœ ì§€
                refined.append(reviewed)
            else:
                print(f"{c['courseName']} ëª¨ë“  ì„¸ì…˜ì´ ì œê±°ë˜ì–´ ì œì™¸ë¨")
        filtered = refined

        # === 5. ì„¸ì…˜ ê°œìˆ˜ê°€ ë„ˆë¬´ ì ì€ ì½”ìŠ¤ ì œê±° === 
        final_courses = []
        removed_short = []
        for c in filtered:
            if len(c.get("sessions", [])) < 3:
                removed_short.append({
                    "courseName": c["courseName"],
                    "reason": f"ì„¸ì…˜ {len(c.get('sessions', []))}ê°œë¡œ, í•™ìŠµìš© ê¸°ì¤€(3ê°œ) ë¯¸ë‹¬"
                })
            else:
                final_courses.append(c)
        filtered = final_courses

        if removed_short:
            print(f"\n=== ì„¸ì…˜ ë¶€ì¡±ìœ¼ë¡œ ì œê±°ëœ ì½”ìŠ¤ ({len(removed_short)}ê°œ) ===")
            for r in removed_short:
                print(f"  - {r['courseName']} ({r['reason']})")
            print("-" * 50)

        # === 6. ì½”ìŠ¤ë³„ ì ìˆ˜ ì¶œë ¥ ===
        print("\n=== ì½”ìŠ¤ë³„ ì„¸ì…˜ ì¼ê´€ì„± ì ìˆ˜ ===")
        for s in coherence_scores:
            print(f"  - {s['courseName']}: {s['score']:.3f}")
        print("===============================")

        # === 7. ê°€ì¥ ì¼ê´€ì„± ë‚®ì€ ì½”ìŠ¤ í‘œì‹œ ===
        worst = coherence_scores[0] if coherence_scores else None
        if worst:
            print(f"\nì¼ê´€ì„± ê°€ì¥ ë‚®ì€ ì½”ìŠ¤: {worst['courseName']} ({worst['score']:.3f})")

        # === 8. ì¸ë±ìŠ¤ ì¬ì •ë ¬ ===
        reindexed_filtered = reindex_courses(filtered)

        # ë¶ˆí•„ìš”í•œ similarity í•„ë“œ ì œê±°ëŠ” reindexed_filtered ëŒ€ìƒìœ¼ë¡œ ìˆ˜í–‰
        for c in reindexed_filtered:
            for s in c.get("sessions", []):
                s.pop("similarity_to_center", None)

        # ì •ì¹˜ ì½”ìŠ¤(ì›ë³¸) + ì •ì œëœ ë¹„ì •ì¹˜ ì½”ìŠ¤ ë³‘í•©
        merged = reindexed_filtered

        # === 9. ê²°ê³¼ ì €ì¥ ===
        out_path = FILTER_DIR / f"{file.name}"
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(merged, f, ensure_ascii=False, indent=2)
        print(f"\ní•„í„°ë§ ë° ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ â†’ {out_path.name}")

    print("\nëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ.")

if __name__ == "__main__":
    print("=== ë‰´ìŠ¤ ì½”ìŠ¤ í•„í„°ë§ ì‹œì‘ ===")
    main()
    print("=== ëª¨ë“  ì½”ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ ===")