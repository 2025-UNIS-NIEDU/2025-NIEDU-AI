import os, json, logging
from pathlib import Path
import chromadb
from chromadb.utils import embedding_functions
from dotenv import load_dotenv
from collections import OrderedDict
from datetime import datetime

log = logging.getLogger(__name__) 

def build_rag_data():
    # 0ï¸. ê¸°ë³¸ ì„¤ì •
    today = datetime.now().strftime("%Y-%m-%d")

    BASE_DIR = Path(__file__).resolve().parents[2]
    ENV_PATH = BASE_DIR / ".env"
    BACKUP_DIR = BASE_DIR / "data" / "backup"
    DB_ROOT = BASE_DIR / "data" / "rag_db"

    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    DB_ROOT.mkdir(parents=True, exist_ok=True)

    # 1ï¸. í™˜ê²½ ë³€ìˆ˜ ë° ì„ë² ë”© ì„¤ì •
    load_dotenv(dotenv_path=ENV_PATH, override=True)
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="jhgan/ko-sroberta-multitask"
    )

    # 2ï¸. í‚¤ ì •ë ¬ í•¨ìˆ˜ (deepsearchId, topic, subTopic ìš°ì„ )
    def sort_deepsearchId_keys(session: dict) -> dict:
        """deepsearchId â†’ topic â†’ subTopic ìˆœìœ¼ë¡œ ê³ ì •, ë‚˜ë¨¸ì§€ëŠ” ì•ŒíŒŒë²³ ìˆœ ì •ë ¬"""
        if not isinstance(session, dict):
            return session

        ordered = []
        for key in ("deepsearchId", "topic", "subTopic"):
            if key in session:
                ordered.append((key, session[key]))

        remaining = sorted(
            [(k, v) for k, v in session.items() if k not in ("deepsearchId", "topic", "subTopic")],
            key=lambda x: x[0].lower()
        )

        return dict(ordered + remaining)

    # 3ï¸. ë°±ì—… í´ë” ë‚´ JSON íŒŒì¼ ì¤‘ ìµœì‹  ë‚ ì§œë§Œ ìœ ì§€
    json_files_all = sorted(BACKUP_DIR.glob("*.json"), key=os.path.getmtime, reverse=True)
    if not json_files_all:
        log.warning("ë°±ì—… í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    latest_date = json_files_all[0].stem.split("_")[-1].split(".")[0]
    json_files = [f for f in json_files_all if latest_date in f.name]
    log.info(f"ğŸ“… ìµœì‹  ë‚ ì§œ({latest_date}) ê¸°ì¤€ {len(json_files)}ê°œ JSON ì²˜ë¦¬")

    # ê¸°ì¡´ DB ì´ˆê¸°í™” (ì»¬ë ‰ì…˜ ë‹¨ìœ„ ì‚­ì œ)
    for topic_dir in DB_ROOT.iterdir():
        if topic_dir.is_dir():
            client = chromadb.PersistentClient(path=str(topic_dir))
            collection_name = f"{topic_dir.name}_news"
            try:
                client.delete_collection(name=collection_name)
                log.info(f"[{topic_dir.name}] ê¸°ì¡´ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                log.warning(f"[{topic_dir.name}] ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # 4ï¸. ìµœì‹  JSON íŒŒì¼ë³„ ë³€í™˜ ë° DB ì €ì¥
    for json_file in json_files:
        topic_name = json_file.stem.split("_")[0]
        topic_db_path = DB_ROOT / topic_name
        topic_db_path.mkdir(parents=True, exist_ok=True)

        client = chromadb.PersistentClient(path=str(topic_db_path))
        collection = client.get_or_create_collection(
            name=f"{topic_name}_news",
            embedding_function=embedding_fn
        )

        # === JSON ë¡œë“œ ===
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                raw = json.load(f)
            articles = raw.get("articles", raw.get("data", []))
        except Exception as e:
            log.error(f"[{topic_name}] JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

        docs, metas, ids = [], [], []

        # 5ï¸. ê¸°ì‚¬ë³„ ë¬¸ì„œ ë³€í™˜
        for i, item in enumerate(articles):
            try:
                ordered_item = sort_deepsearchId_keys(item)

                clean_dict = OrderedDict()
                for k, v in ordered_item.items():
                    if v is None:
                        v = ""
                    elif isinstance(v, (list, dict)):
                        v = json.dumps(v, ensure_ascii=False)
                    else:
                        v = str(v)
                    clean_dict[k] = v

                page_content = f"""
                [ê¸°ì‚¬ ì œëª©] {clean_dict.get('headline', '')}

                [ì£¼ì œ] {clean_dict.get('topic', '')}
                [ì„¸ë¶€ ì£¼ì œ] {clean_dict.get('subTopic', '')}

                [ìš”ì•½] {clean_dict.get('summary', '')}

                [ì–¸ë¡ ì‚¬] {clean_dict.get('publisher', '')}
                [ê²Œì‹œì¼] {clean_dict.get('publishedAt', '')}

                [ê¸°ì‚¬ URL] {clean_dict.get('contentUrl', '')}
                [ì¸ë„¤ì¼] {clean_dict.get('thumbnailUrl', '')}

                [DeepSearch ID] {clean_dict.get('deepsearchId', '')}
                """

                docs.append(page_content.strip())
                metas.append(clean_dict)
                ids.append(f"{topic_name}_{i+1}")

            except Exception as e:
                log.warning(f"[{topic_name}] ë¬¸ì„œ {i} ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

        # 6ï¸. DBì— ì €ì¥
        if docs:
            try:
                collection.add(documents=docs, metadatas=metas, ids=ids)
                log.info(f"[{topic_name}] {len(docs)}ê°œ ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ")
            except Exception as e:
                log.error(f"[{topic_name}] DB ì €ì¥ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")

    # 7ï¸. subTopic ê²€ì¦
    for topic_dir in DB_ROOT.iterdir():
        if topic_dir.is_dir():
            client = chromadb.PersistentClient(path=str(topic_dir))
            collection = client.get_or_create_collection(
                name=f"{topic_dir.name}_news",
                embedding_function=embedding_fn
            )
            data = collection.get(include=["metadatas"])
            if not data["metadatas"]:
                continue
            subs = [m.get("subTopic", None) for m in data["metadatas"] if m]
            unique_subs = set(s for s in subs if s)
            log.info(f"[{topic_dir.name}] subTopic {len(unique_subs)}ê°œ í™•ì¸")

    # 8. RAG DB ë¬¸ì„œ ìˆ˜ í™•ì¸
    for topic_dir in DB_ROOT.iterdir():
        if topic_dir.is_dir():
            client = chromadb.PersistentClient(path=str(topic_dir))
            collection = client.get_or_create_collection(
                name=f"{topic_dir.name}_news"
            )
            count = collection.count()
            log.info(f"[{topic_dir.name}] ë¬¸ì„œ ìˆ˜: {count}")

#  ì‹¤í–‰
if __name__ == "__main__":
    build_rag_data()