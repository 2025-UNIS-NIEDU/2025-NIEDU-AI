import os
import json
from pathlib import Path
import chromadb
from chromadb.utils import embedding_functions
from dotenv import load_dotenv
from collections import OrderedDict
from datetime import datetime

def build_rag_data():
    # 0ï¸. ê¸°ë³¸ ì„¤ì •
    today = datetime.now().strftime("%Y-%m-%d")

    BASE_DIR = Path(__file__).resolve().parents[2]
    ENV_PATH = BASE_DIR / ".env"
    BACKUP_DIR = BASE_DIR / "data" / "backup"
    DB_ROOT = BASE_DIR / "data" / "rag_db"

    BACKUP_DIR.mkdir(parents=True, exist_ok=True)
    DB_ROOT.mkdir(parents=True, exist_ok=True)

    # 1ï¸. í™˜ê²½ ë³€ìˆ˜ ë° ì„ë² ë”© ì„¤ì •
    load_dotenv(dotenv_path=ENV_PATH, override=True)
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="jhgan/ko-sroberta-multitask"
    )

    # 2ï¸. í‚¤ ì •ë ¬ í•¨ìˆ˜ (deepsearchId, topic, subTopic ìš°ì„ )
    def sort_deepsearchId_keys(session: dict) -> dict:
        """deepsearchId â†’ topic â†’ subTopic ìˆœìœ¼ë¡œ ê³ ì •, ë‚˜ë¨¸ì§€ëŠ” ì•ŒíŒŒë²³ ìˆœ ì •ë ¬"""
        if not isinstance(session, dict):
            return session

        ordered = []
        for key in ("deepsearchId", "topic", "subTopic"):
            if key in session:
                ordered.append((key, session[key]))

        remaining = sorted(
            [(k, v) for k, v in session.items() if k not in ("deepsearchId", "topic", "subTopic")],
            key=lambda x: x[0].lower()
        )

        return dict(ordered + remaining)

    # 3ï¸. ë°±ì—… í´ë” ë‚´ JSON íŒŒì¼ ì¤‘ ìµœì‹  ë‚ ì§œë§Œ ìœ ì§€
    json_files_all = sorted(BACKUP_DIR.glob("*.json"), key=os.path.getmtime, reverse=True)

    if not json_files_all:
        print("âš ï¸ ë°±ì—… í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ìµœì‹  ë‚ ì§œ ì¶”ì¶œ
    latest_date = json_files_all[0].stem.split("_")[-1].split(".")[0]
    json_files = [f for f in json_files_all if latest_date in f.name]

    print(f"ğŸ“… ìµœì‹  ë‚ ì§œ({latest_date}) ê¸°ì¤€ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # ê¸°ì¡´ DB ì´ˆê¸°í™” (ëª¨ë“  í† í”½ DB ì‚­ì œ)
    for topic_dir in DB_ROOT.iterdir():
        if topic_dir.is_dir():
            print(f"ğŸ§¹ {topic_dir.name} ê¸°ì¡´ DB ì‚­ì œ ì¤‘...")
            for file in topic_dir.glob("*"):
                file.unlink()
            topic_dir.rmdir()

    # 4ï¸. ìµœì‹  JSON íŒŒì¼ë³„ ë³€í™˜ ë° DB ì €ì¥
    for json_file in json_files:
        topic_name = json_file.stem.split("_")[0]
        print(f"\n[{topic_name}] ë³€í™˜ ì¤‘ (íŒŒì¼: {json_file.name}) ...")

        topic_db_path = DB_ROOT / topic_name
        topic_db_path.mkdir(parents=True, exist_ok=True)

        client = chromadb.PersistentClient(path=str(topic_db_path))
        collection = client.get_or_create_collection(
            name=f"{topic_name}_news",
            embedding_function=embedding_fn
        )

        # === JSON ë¡œë“œ ===
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                raw = json.load(f)
            articles = raw.get("articles", raw.get("data", []))
        except Exception as e:
            print(f"[ì˜¤ë¥˜] {topic_name} JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

        docs, metas, ids = [], [], []

        # 5ï¸. ê¸°ì‚¬ë³„ ë¬¸ì„œ ë³€í™˜
        for i, item in enumerate(articles):
            try:
                ordered_item = sort_deepsearchId_keys(item)

                # NoneType, list, dict ë°©ì§€
                clean_dict = OrderedDict()
                for k, v in ordered_item.items():
                    if v is None:
                        v = ""
                    elif isinstance(v, (list, dict)):
                        v = json.dumps(v, ensure_ascii=False)
                    else:
                        v = str(v)
                    clean_dict[k] = v

                # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ êµ¬ì„± (ìì—°ì–´ ê¸°ë°˜)
                page_content = f"""
                [ê¸°ì‚¬ ì œëª©] {clean_dict.get('headline', '')}

                [ì£¼ì œ] {clean_dict.get('topic', '')}
                [ì„¸ë¶€ ì£¼ì œ] {clean_dict.get('subTopic', '')}

                [ìš”ì•½] {clean_dict.get('summary', '')}

                [ì–¸ë¡ ì‚¬] {clean_dict.get('publisher', '')}
                [ê²Œì‹œì¼] {clean_dict.get('publishedAt', '')}

                [ê¸°ì‚¬ URL] {clean_dict.get('contentUrl', '')}
                [ì¸ë„¤ì¼] {clean_dict.get('thumbnailUrl', '')}

                [DeepSearch ID] {clean_dict.get('deepsearchId', '')}
                """

                docs.append(page_content.strip())  # ì‹¤ì œ ì„ë² ë”©ì— ì‚¬ìš©
                metas.append(clean_dict)           # ì „ì²´ ë©”íƒ€ë°ì´í„° ì €ì¥
                ids.append(f"{topic_name}_{i+1}")

            except Exception as e:
                print(f"[ê²½ê³ ] {topic_name} ë¬¸ì„œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # 6ï¸. DBì— ì €ì¥
        if docs:
            try:
                collection.add(documents=docs, metadatas=metas, ids=ids)
                print(f"{len(docs)}ê°œ ë¬¸ì„œë¥¼ DBì— ì¶”ê°€ ì™„ë£Œ â†’ {topic_db_path}")
                print(f"í˜„ì¬ ì»¬ë ‰ì…˜ ë¬¸ì„œ ìˆ˜: {collection.count()}")

                # ì¶”ê°€ëœ ë°ì´í„° ìƒ˜í”Œ 1~2ê°œë§Œ í™•ì¸
                sample = collection.get(limit=2, include=["documents", "metadatas"])
                print(f"ìƒ˜í”Œ ë¬¸ì„œ 2ê°œ ë¯¸ë¦¬ë³´ê¸° ({topic_name}):")
                for idx, doc in enumerate(sample["documents"]):
                    meta = sample["metadatas"][idx]
                    print(f"  â”œâ”€ [{meta.get('subTopic', 'N/A')}] {meta.get('headline', '')[:50]}...")
                print("-" * 80)

            except Exception as e:
                print(f"[ì˜¤ë¥˜] {topic_name} DB ì €ì¥ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        else:
            print(f"[ì£¼ì˜] {topic_name}ì— ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    # 7ï¸. subTopic ê²€ì¦
    print("\n=== subTopic í•„ë“œ ê²€ì¦ ===")
    for topic_dir in DB_ROOT.iterdir():
        if topic_dir.is_dir():
            client = chromadb.PersistentClient(path=str(topic_dir))
            collection = client.get_or_create_collection(
                name=f"{topic_dir.name}_news",
                embedding_function=embedding_fn
            )
            data = collection.get(include=["metadatas"])
            if not data["metadatas"]:
                print(f"{topic_dir.name}: ë©”íƒ€ë°ì´í„° ì—†ìŒ")
                continue

            subs = [m.get("subTopic", None) for m in data["metadatas"] if m]
            unique_subs = set(s for s in subs if s)
            print(f"{topic_dir.name}: subTopic í•„ë“œ í™•ì¸ ({len(unique_subs)}ê°œ ê³ ìœ ê°’)")
            print(f"ì˜ˆì‹œ: {list(unique_subs)[:5]}")

    print("\nëª¨ë“  í† í”½ DB ì €ì¥ ë° ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 8. RAG DB ë¬¸ì„œ ìˆ˜ í™•ì¸
    print("\n=== ê° í† í”½ë³„ ë¬¸ì„œ ìˆ˜ í™•ì¸ ===")
    BASE_DIR = Path(__file__).resolve().parents[2]
    DB_ROOT = BASE_DIR / "data" / "rag_db"

    for topic_dir in DB_ROOT.iterdir():
        if topic_dir.is_dir():
            client = chromadb.PersistentClient(path=str(topic_dir))
            collection = client.get_or_create_collection(
                name=f"{topic_dir.name}_news"
            )
            count = collection.count()
            print(f"[{topic_dir.name}] ë¬¸ì„œ ìˆ˜: {count}")

#  ì‹¤í–‰
if __name__ == "__main__":
    build_rag_data()