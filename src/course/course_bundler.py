import os
import json
from pathlib import Path
import chromadb
from chromadb.utils import embedding_functions
from dotenv import load_dotenv
from openai import OpenAI
from datetime import datetime
from collections import OrderedDict, defaultdict

# === ë‚ ì§œ ===
today = datetime.now().strftime("%Y-%m-%d")

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = Path(__file__).resolve().parents[2]
ENV_PATH = BASE_DIR / ".env"
COURSE_DIR = BASE_DIR / "data" / "course"
COURSE_DIR.mkdir(parents=True, exist_ok=True)

# === í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ===
load_dotenv(ENV_PATH, override=True)
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

# === session ì •ë ¬ í•¨ìˆ˜ ===
def sort_session_keys(session: dict) -> dict:
    """sessionId â†’ topic â†’ subTopic â†’ ë‚˜ë¨¸ì§€ ì•ŒíŒŒë²³ìˆœ ì •ë ¬"""
    if not isinstance(session, dict):
        return session

    ordered = []
    for k in ("sessionId", "topic", "subTopic"):
        if k in session:
            ordered.append((k, session[k]))
    remaining = sorted(
        [(k, v) for k, v in session.items() if k not in ("sessionId", "topic", "subTopic")],
        key=lambda x: x[0].lower()
    )
    return OrderedDict(ordered + remaining)

# === í† í”½ ëª©ë¡ ===
TOPIC_SUBTOPICS = {
    "politics": "ëŒ€í†µë ¹ì‹¤ OR êµ­íšŒ OR ì •ë‹¹ OR ë¶í•œ OR êµ­ë°© OR ì™¸êµ OR ë²•ë¥ ",
    "economy": "ê¸ˆìœµ OR ì¦ê¶Œ OR ì‚°ì—… OR ì¤‘ì†Œê¸°ì—… OR ë¶€ë™ì‚° OR ë¬¼ê°€ OR ë¬´ì—­",
    "society": "ì‚¬ê±´ OR êµìœ¡ OR ë…¸ë™ OR í™˜ê²½ OR ì˜ë£Œ OR ë³µì§€ OR ì  ë”",
    "world": "ë¯¸êµ­ OR ì¤‘êµ­ OR ì¼ë³¸ OR ìœ ëŸ½ OR ì¤‘ë™ OR ì•„ì‹œì•„ OR êµ­ì œ",
    "tech": "ì¸ê³µì§€ëŠ¥ OR ë°˜ë„ì²´ OR ë¡œë´‡ OR ë””ì§€í„¸ OR ê³¼í•™ê¸°ìˆ  OR ì—°êµ¬ê°œë°œ OR í˜ì‹ "
}

# === ì„ë² ë”© í•¨ìˆ˜ ===
embedding_fn = embedding_functions.OpenAIEmbeddingFunction(
    api_key=api_key, model_name="text-embedding-3-small"
)

# === ë©”ì¸ ì²˜ë¦¬ ===
for topic in TOPIC_SUBTOPICS.keys():
    print(f"\n[{topic}] ChromaDB ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")

    DB_DIR = BASE_DIR / "data" / "db" / topic
    chroma_client = chromadb.PersistentClient(path=str(DB_DIR))

    # --- ì»¬ë ‰ì…˜ ê°ì§€ ---
    collections = chroma_client.list_collections()
    if not collections:
        print(f"[ê²½ê³ ] {topic}: ì»¬ë ‰ì…˜ ì—†ìŒ. ìŠ¤í‚µ.")
        continue

    # v0.6.0 ëŒ€ì‘ (ë¬¸ìì—´ or dict)
    collection_name = (
        collections[0] if isinstance(collections[0], str)
        else collections[0].get("name", f"{topic}_news")
    )

    collection = chroma_client.get_collection(
        name=collection_name,
        embedding_function=embedding_fn
    )
    print(f"ğŸ“‚ ê°ì§€ëœ ì»¬ë ‰ì…˜: {collection_name}")

    # --- ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ---
    try:
        all_data = collection.get(include=["metadatas"], limit=5000)
        metadatas = all_data.get("metadatas", [])
    except Exception as e:
        print(f"[ì˜¤ë¥˜] {topic} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        continue

    if not metadatas:
        print(f"{topic} â€” ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œ ì—†ìŒ (docs ë¹„ì–´ìˆìŒ)")
        continue

    docs = []
    for meta in metadatas:
        if not isinstance(meta, dict):
            continue
        filtered_meta = {k: v for k, v in meta.items() if k != "deepsearchId"}
        docs.append(sort_session_keys(filtered_meta))

    print(f"{topic} ë‰´ìŠ¤ ê°œìˆ˜: {len(docs)}")

    # --- ìƒ˜í”Œ ë¯¸ë¦¬ë³´ê¸° ---
    preview_docs = docs[:2]
    if preview_docs:
        print(f"ìƒ˜í”Œ ë¬¸ì„œ {len(preview_docs)}ê°œ ë¯¸ë¦¬ë³´ê¸° ({topic}):")
        for d in preview_docs:
            print(f"  â”œâ”€ [{d.get('subTopic', 'ë¯¸ì§€ì •')}] {d.get('headline', '')[:60]}...")
        print("-" * 80)

    # --- subTopicë³„ ê·¸ë£¹í™” ---
    grouped_by_sub = defaultdict(list)
    for d in docs:
        subTopic = d.get("subTopic", "ê¸°íƒ€") or "ê¸°íƒ€"
        grouped_by_sub[subTopic].append(d)

    output = []

    # --- ê° subTopic ë‹¨ìœ„ë¡œ ì½”ìŠ¤ ìƒì„± ---
    for idx, (subTopic, group_news) in enumerate(grouped_by_sub.items(), start=1):
        # ì„¸ì…˜ ID ë¶€ì—¬ ë° ì •ë ¬
        for sid, news in enumerate(group_news, start=1):
            news["sessionId"] = sid
            group_news[sid - 1] = sort_session_keys(news)

        cleaned_sessions = []
        for s in group_news:
            filtered = {k: v for k, v in s.items() if k not in ("topic", "subTopic", "deepsearchId")}
            cleaned_sessions.append(sort_session_keys(filtered))    

        # ë‰´ìŠ¤ ìš”ì•½ ê²°í•©
        summaries = [
            f"- {n.get('headline', '')}: {n.get('summary', '')[:400]}"
            for n in group_news
        ]
        joined_summary = "\n".join(summaries)

        # --- ì½”ìŠ¤ ì„¤ëª… ìƒì„± ---
        prompt_course = f"""
ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ë‹¹ì‹ ì€ í•™ìŠµí˜• ë‰´ìŠ¤ ì½˜í…ì¸ ë¥¼ ê¸°íší•˜ëŠ” ì‘ê°€ì´ì ì—ë””í„°ì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” ë¹„ìŠ·í•œ ì£¼ì œì˜ ë‰´ìŠ¤ ì—¬ëŸ¬ ê°œì…ë‹ˆë‹¤.
ì´ ë‰´ìŠ¤ë“¤ì˜ íë¦„ì„ ë°”íƒ•ìœ¼ë¡œ, **ê°€ë…ì„±ì´ ë†’ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì½”ìŠ¤ ì œëª©(courseName)**ê³¼
ì§§ì§€ë§Œ íë¦„ì´ ë³´ì´ëŠ” **ì½”ìŠ¤ ì„¤ëª…(courseDescription)**ì„ í•¨ê»˜ ì‘ì„±í•˜ì„¸ìš”.
ì½”ìŠ¤ ì„¤ëª…ì€ 80-100ì ë‚´ì™¸

ì¶œë ¥ í˜•ì‹(JSON):
{{
  "courseName": string,
  "courseDescription": string
}}

ë‰´ìŠ¤ ìƒ˜í”Œ ìš”ì•½:
{joined_summary}
"""
        try:
            resp_course = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt_course}],
                response_format={"type": "json_object"}
            )
            meta_course = json.loads(resp_course.choices[0].message.content)
        except Exception as e:
            print(f"[{topic}] {subTopic} course ìƒì„± ì‹¤íŒ¨: {e}")
            meta_course = {
                "courseName": f"{topic}_{subTopic}",
                "courseDescription": "ìë™ ìƒì„± ì‹¤íŒ¨ â†’ ê¸°ë³¸ ì„¤ëª…"
            }

        # --- íƒœê·¸ ---
        tags = [topic]
        if subTopic:
            tags.append(f"#{subTopic}")

        # --- ìµœì¢… ì½”ìŠ¤ êµ¬ì„± ---
        course_data = OrderedDict([
            ("courseId", idx),
            ("tags", tags),
            ("courseName", meta_course.get("courseName", f"{topic}_{subTopic}")),
            ("courseDescription", meta_course.get("courseDescription", "")),
            ("sessions", cleaned_sessions),
        ])
        output.append(course_data)

    # --- ì €ì¥ ---
    output_sorted = sorted(output, key=lambda x: x["courseId"])
    output_file = COURSE_DIR / f"{topic}_{today}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(output_sorted, f, ensure_ascii=False, indent=2, sort_keys=False)

    print(f"ğŸ’¾ {topic} â†’ ì½”ìŠ¤ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file.resolve()}")

print("\nğŸ‰ ëª¨ë“  í† í”½ ì½”ìŠ¤ íŒŒì¼ ìƒì„± ì™„ë£Œ.")