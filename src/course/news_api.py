import os
import re
import requests
import json
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime
import time

# === ë‚ ì§œ ì •ì˜ ===
today = datetime.now().strftime("%Y-%m-%d")

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = Path(__file__).resolve().parents[2]
ENV_PATH = BASE_DIR / ".env"
BACKUP_DIR = BASE_DIR / "data" / "backup"
BACKUP_DIR.mkdir(parents=True, exist_ok=True)

# === í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ===
load_dotenv(dotenv_path=ENV_PATH, override=True)
DEEPSEARCH_API_KEY = os.getenv("DEEPSEARCH_API_KEY")

# === í•œê¸€ ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜ ===
def is_purely_korean(text: str, threshold: float = 0.3) -> bool:
    """
    ì˜ì–´ì™€ í•œìê°€ í¬í•¨ë˜ì§€ ì•Šê³ ,
    í•œê¸€ ë¹„ìœ¨ì´ ì¼ì • ì´ìƒì¼ ë•Œë§Œ True ë°˜í™˜.
    """
    if not text:
        return False

    # ì˜ì–´ ë˜ëŠ” í•œì í¬í•¨ ì‹œ ë°”ë¡œ False
    if re.search(r"[A-Za-z]", text) or re.search(r"[\u3400-\u4DBF\u4E00-\u9FFF]", text):
        return False

    # í•œê¸€ ë¹„ìœ¨ ê³„ì‚°
    korean_chars = len(re.findall(r"[ê°€-í£]", text))
    total_chars = len(text)

    return (korean_chars / total_chars) >= threshold if total_chars > 0 else False

# === DeepSearch í˜¸ì¶œ í•¨ìˆ˜ ===
def deepsearch_query(endpoint: str, subTopic: str, date_from: str, date_to: str,
                     page: int = 10, page_size: int = 70, order="published_at", direction="desc"):
    """DeepSearch API í˜¸ì¶œ"""
    params = {
        "api_key": DEEPSEARCH_API_KEY,
        "q": subTopic,
        "page": page,
        "page_size": page_size,
        "date_from": date_from,
        "date_to": date_to,
        "order": order,
        "direction": direction
    }
    resp = requests.get(endpoint, params=params)
    resp.raise_for_status()
    return resp.json()

# === deepsearchId, topic ê³ ì • + ë‚˜ë¨¸ì§€ ì•ŒíŒŒë²³ ìˆœ ===
def sort_session_keys(session: dict) -> dict:
    """deepsearchId â†’ topic ê³ ì •, ë‚˜ë¨¸ì§€ëŠ” ì•ŒíŒŒë²³ ìˆœìœ¼ë¡œ ì •ë ¬"""
    if not isinstance(session, dict):
        return session

    ordered = []
    # 1ï¸. deepsearchId, topic ë¨¼ì €
    if "deepsearchId" in session:
        ordered.append(("deepsearchId", session["deepsearchId"]))
    if "topic" in session:
        ordered.append(("topic", session["topic"]))

    # 2ï¸. ë‚˜ë¨¸ì§€ í‚¤ ì•ŒíŒŒë²³ ìˆœ ì •ë ¬
    remaining = sorted(
        [(k, v) for k, v in session.items() if k not in ("deepsearchId", "topic")],
        key=lambda x: x[0].lower()
    )

    return dict(ordered + remaining)

# === ê¸°ì‚¬ ìˆ˜ì§‘ (ë‹¤ì¤‘ í˜ì´ì§€ + í•„í„°ë§) ===
def collect_articles_with_filter(topic: str, subTopic: str,
                                 date_from: str, date_to: str,
                                 min_length: int = 350, target_samples: int = 10,
                                 max_pages: int = 30):
    """
    í† í”½ë³„ ê¸°ì‚¬ ìˆ˜ì§‘ (ê¸¸ì´ + ì˜ë¦¼ + ì˜ì–´/í•œì ì œì™¸)
    ê° subTopicì„ ìµœëŒ€ max_pages(ê¸°ë³¸ 10í˜ì´ì§€)ê¹Œì§€ í˜¸ì¶œí•˜ì—¬ ìˆ˜ì§‘
    """
    endpoint = f"https://api-v2.deepsearch.com/v1/articles/{topic}"
    collected = []

    for page in range(1, max_pages + 1):
        print(f"  â†³ {topic}/{subTopic} | í˜ì´ì§€ {page} í˜¸ì¶œ ì¤‘...")

        try:
            resp = deepsearch_query(
                endpoint,
                subTopic=subTopic,
                date_from=date_from,
                date_to=date_to,
                page=page,  
                page_size=target_samples,  
                order="published_at",
                direction="desc"
            )
        except Exception as e:
            print(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

        articles = resp.get("data", [])
        if not articles:
            print(f"í˜ì´ì§€ {page}: ë°ì´í„° ì—†ìŒ â†’ ì¤‘ë‹¨")
            break

        for a in articles:
            summary = (a.get("summary") or "").strip()

            # 1ï¸. ë„ˆë¬´ ì§§ì€ ê²½ìš° ì œì™¸
            if len(summary) < min_length:
                continue

            # 2ï¸. '...' ë˜ëŠ” 'â€¦'ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš° ì œì™¸ (ìš”ì•½ ì˜ë¦¼)
            if summary.endswith("...") or summary.endswith("â€¦"):
                continue

            # 3ï¸. ì˜ì–´Â·í•œì í¬í•¨ ê¸°ì‚¬ ì œì™¸
            if not is_purely_korean(summary):
                continue

            # ë‰´ìŠ¤ ë°ì´í„° ì •ë¦¬
            session = {
                "deepsearchId": a.get("id"),
                "topic": topic,
                "subTopic": subTopic,
                "summary": summary,
                "contentUrl": a.get("content_url"),
                "headline": a.get("title"),
                "publishedAt": a.get("published_at"),
                "publisher": a.get("publisher"),
                "thumbnailUrl": a.get("thumbnail_url")
            }

            # deepsearchId/topic ë¨¼ì € ì •ë ¬
            session = sort_session_keys(session)
            collected.append(session)

            # ğŸ¯ ëª©í‘œ ê°œìˆ˜ ë„ë‹¬ ì‹œ ì¡°ê¸° ì¢…ë£Œ
            if len(collected) >= target_samples:
                print(f"{topic}/{subTopic}: ëª©í‘œ {target_samples}ê±´ ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨")
                break

        # í˜ì´ì§€ ê°„ ë”œë ˆì´
        time.sleep(1)

        # ìˆ˜ì§‘ ì™„ë£Œ ì‹œ ë£¨í”„ ì¢…ë£Œ
        if len(collected) >= target_samples:
            break

    print(f"{topic}/{subTopic}: ì´ {len(collected)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (í•„í„°ë§ í›„)")
    return collected

# === í† í”½ë³„ subTopic ì •ì˜ ===
TOPIC_SUBTOPICS = {
    "politics": "ëŒ€í†µë ¹ì‹¤ OR êµ­íšŒ OR ì •ë‹¹ OR ë¶í•œ OR êµ­ë°© OR ì™¸êµ OR ë²•ë¥ ",
    "economy": "ê¸ˆìœµ OR ì¦ê¶Œ OR ì‚°ì—… OR ì¤‘ì†Œê¸°ì—… OR ë¶€ë™ì‚° OR ë¬¼ê°€ OR ë¬´ì—­",
    "society": "ì‚¬ê±´ OR êµìœ¡ OR ë…¸ë™ OR í™˜ê²½ OR ì˜ë£Œ OR ë³µì§€ OR ì  ë”",
    "world": "ë¯¸êµ­ OR ì¤‘êµ­ OR ì¼ë³¸ OR ìœ ëŸ½ OR ì¤‘ë™ OR ì•„ì‹œì•„ OR êµ­ì œ",
    "tech": "ì¸ê³µì§€ëŠ¥ OR ë°˜ë„ì²´ OR ë¡œë´‡ OR ë””ì§€í„¸ OR ê³¼í•™ê¸°ìˆ  OR ì—°êµ¬ê°œë°œ OR í˜ì‹ "
}

# === ì‹¤í–‰ ===
date_from = "2025-10-20"
date_to = "2025-10-21"

for topic, subTopics in TOPIC_SUBTOPICS.items():
    print(f"\n=== [{topic}] ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘... ===")
    collected_all = []

    # "OR"ë¡œ êµ¬ë¶„ëœ subTopic ê°ê° ì¿¼ë¦¬ë¡œ ë¶„ë¦¬
    subTopic_list = [s.strip() for s in re.split(r"\s*OR\s*", subTopics) if s.strip()]

    for sub in subTopic_list:
        print(f"--- ğŸ” SubTopic ì¿¼ë¦¬: {sub} ---")
        try:
            cleaned = collect_articles_with_filter(
                topic=topic,
                subTopic=sub,
                date_from=date_from,
                date_to=date_to,
                min_length=320,
                target_samples=10 
            )
            collected_all.extend(cleaned)
        except Exception as e:
            print(f"[ì˜¤ë¥˜ ë°œìƒ] {topic}/{sub}: {e}")
        time.sleep(2)

    backup_file = BACKUP_DIR / f"{topic}_{today}.json"
    with open(backup_file, "w", encoding="utf-8") as f:
        json.dump({"topic": topic, "articles": collected_all}, f, ensure_ascii=False, indent=2)

    print(f"[ì •ì œ ì™„ë£Œ] {backup_file.resolve()} | {len(collected_all)}ê±´ ì €ì¥ ì™„ë£Œ")