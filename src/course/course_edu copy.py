# === í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ===
import os, json, time
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# === ê²½ë¡œ ì„¤ì • ===
BASE_DIR = Path(__file__).resolve().parents[2]
ENV_PATH = BASE_DIR / ".env"
COURSE_DIR = BASE_DIR / "data" / "course_db"
FILTER_DIR = COURSE_DIR / "filtered"
FILTER_DIR.mkdir(parents=True, exist_ok=True)

# === í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ===
load_dotenv(ENV_PATH, override=True)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# === ì„ë² ë”© ëª¨ë¸ (í•œë²ˆë§Œ ë¡œë“œ) ===
embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")

# === LLM í”„ë¡¬í”„íŠ¸ ===
PROMPT_FILTER = """
ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ë°˜ í•™ìŠµ ì½”ìŠ¤ì˜ ì„¸ì…˜ ê²€ìˆ˜ìì´ë‹¤.
ë‹¤ìŒ ì„¸ì…˜(ë‰´ìŠ¤ ì œëª©) ì¤‘ ì •ë§ë¡œ ì£¼ì œì™€ ì „í˜€ ê´€ê³„ì—†ëŠ” ê²ƒë§Œ ê³¨ë¼ë¼.

íŒë‹¨ ê¸°ì¤€:
- ì‚¬íšŒ, ì •ì¹˜, ê²½ì œ, ì™¸êµ, ê¸°ìˆ , ì •ì±… ë“± ì‹œì‚¬ íë¦„ê³¼ ê´€ë ¨ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ True(ìœ ì§€)
- ë‹¨ìˆœ ì‚¬ê±´Â·ì‚¬ê³ , ì—°ì˜ˆ, ë‚ ì”¨, ê´‘ê³ , ì¶•ì œ, í¬í† ë‰´ìŠ¤ì²˜ëŸ¼ êµìœ¡ ê°€ì¹˜ê°€ ì „í˜€ ì—†ëŠ” ê²ƒë§Œ False(ì œê±°)
- ì¸ë¬¼ ì¤‘ì‹¬ì´ë¼ë„ ì •ì±…, ë°œì–¸, ì‚¬íšŒì  ì˜í–¥ì´ ìˆìœ¼ë©´ Trueë¡œ ë‘”ë‹¤.

ì¦‰, â€œì´ê±´ ì•„ë¬´ë¦¬ ë´ë„ í•™ìŠµìš©ìœ¼ë¡œ ì“¸ ìˆ˜ ì—†ë‹¤â€ ì‹¶ì€ ê²ƒë§Œ ì œê±°í•˜ë¼.

ì¶œë ¥ í˜•ì‹(JSON Lines):
[
  {"courseName": "...", "is_educational": true or false, "reason": "..."},
  ...
]

ì£¼ì˜ :
1. ì¶œë ¥ ì‹œ ì ˆëŒ€ë¡œ ```json ê°™ì€ ë§ˆí¬ë‹¤ìš´ í‘œì‹œëŠ” ì“°ì§€ ë§ê³ ,
2. ìˆœìˆ˜ JSONë§Œ ë°˜í™˜í•˜ë¼.
"""

# === ì•ˆì „í•œ LLM ìš”ì²­ í•¨ìˆ˜ ===
def safe_request(prompt, retry=3, wait=3):
    for attempt in range(retry):
        try:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ë‰´ìŠ¤ ì½”ìŠ¤ì˜ í’ˆì§ˆ í‰ê°€ìë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
            )
            return resp.choices[0].message.content
        except Exception as e:
            print(f"[ê²½ê³ ] ì¬ì‹œë„ {attempt+1}/{retry}: {e}")
            time.sleep(wait)
    return None


# === ì½”ìŠ¤ëª… ê¸°ë°˜ í•„í„°ë§ ===
def filter_by_course_name(courses, batch_size=8):
    results = []
    for i in tqdm(range(0, len(courses), batch_size), desc="1ë‹¨ê³„: ì½”ìŠ¤ëª… LLM í•„í„°ë§"):
        batch = courses[i:i+batch_size]
        names = "\n".join([f"- {c.get('courseName', '')}" for c in batch])
        prompt = f"{PROMPT_FILTER}\n\n{names}"

        resp = safe_request(prompt)
        if not resp:
            for c in batch:
                results.append({
                    "courseName": c.get("courseName", ""),
                    "is_educational": True,
                    "reason": "API ì‘ë‹µ ì‹¤íŒ¨ (ì„ì‹œ í†µê³¼)"
                })
            continue

        try:
            parsed = json.loads(resp)
            if isinstance(parsed, list):
                results.extend(parsed)
            else:
                results.append(parsed)
        except Exception as e:
            print(f"[íŒŒì‹± ì˜¤ë¥˜] {e}\nì‘ë‹µ:\n{resp[:200]}...\n")
            for c in batch:
                results.append({
                    "courseName": c.get("courseName", ""),
                    "is_educational": True,
                    "reason": "JSON íŒŒì‹± ì‹¤íŒ¨ (ì„ì‹œ í†µê³¼)"
                })
    return results

def compute_coherence(course, noise_threshold=0.4, return_removed=False):
    """
    í•œ ì½”ìŠ¤ ë‚´ headline ì„ë² ë”© ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ ,
    ì¤‘ì‹¬ ë²¡í„°ì™€ì˜ cosine similarityê°€ ë‚®ì€ ì„¸ì…˜ì„ ì œê±°í•œë‹¤.
    return_removed=Trueì¼ ê²½ìš° ì œê±°ëœ headline ëª©ë¡ë„ ë°˜í™˜í•œë‹¤.
    """
    headlines = [s.get("headline", "") for s in course.get("sessions", []) if s.get("headline")]
    if len(headlines) < 2:
        return 0.0 if not return_removed else (0.0, [])

    # === headline ì„ë² ë”© ===
    embs = embedding_model.encode(headlines, convert_to_numpy=True, normalize_embeddings=True)

    # === ì„¸ì…˜ ê°„ í‰ê·  ìœ ì‚¬ë„ ===
    sims_matrix = cosine_similarity(embs)
    upper = np.triu_indices_from(sims_matrix, k=1)
    coherence_score = float(np.mean(sims_matrix[upper]))

    # === ì¤‘ì‹¬ ë²¡í„°(centroid) ê³„ì‚° ===
    centroid = np.mean(embs, axis=0)
    centroid_sims = cosine_similarity(embs, centroid.reshape(1, -1)).flatten()

    # === ë…¸ì´ì¦ˆ ì„¸ì…˜ í•„í„°ë§ ===
    kept_sessions = []
    removed_sessions = []
    for i, s in enumerate(course["sessions"]):
        sim_score = float(centroid_sims[i])
        s["similarity_to_center"] = sim_score
        if sim_score >= noise_threshold:
            kept_sessions.append(s)
        else:
            removed_sessions.append({
                "headline": s.get("headline", ""),
                "similarity": sim_score
            })

    # === ì½”ìŠ¤ ë‚´ ì„¸ì…˜ ì—…ë°ì´íŠ¸ ===
    course["sessions"] = kept_sessions

    if return_removed:
        return coherence_score, removed_sessions
    else:
        return coherence_score
    
PROMPT_SESSION_REVIEW = """
ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ë°˜ í•™ìŠµ ì½”ìŠ¤ì˜ ê²€ìˆ˜ìì´ë‹¤.
ë‹¤ìŒì€ í•œ ì½”ìŠ¤ì˜ ì„¸ì…˜(ë‰´ìŠ¤ ê¸°ì‚¬ ì œëª©) ëª©ë¡ì´ë‹¤.
ì½”ìŠ¤ëª…ê³¼ ì„¸ì…˜ ì œëª©ë“¤ì„ ì½ê³ , ì£¼ì œ íë¦„ì—ì„œ ë²—ì–´ë‚˜ê±°ë‚˜ ê´€ë ¨ì„±ì´ ì•½í•œ ì„¸ì…˜ë§Œ ê³¨ë¼ë¼.

íŒë‹¨ ê¸°ì¤€:
1. ë‹¤ìŒ ìœ í˜•ë§Œ False(ì œê±°)ë¡œ íŒë‹¨í•œë‹¤:
   - ì—°ì˜ˆ, ì‚¬ê±´Â·ì‚¬ê³ , ë²”ì£„, ì¬ë‚œ, ë‹¨ìˆœ êµí†µÂ·í™”ì¬ ë³´ë„
   - ê¸°ì—…Â·ì œí’ˆÂ·ë¸Œëœë“œ í™ë³´, ê´‘ê³ ì„± ê¸°ì‚¬, ì‹ ì œí’ˆ ì¶œì‹œ
   - ì§€ì—­ ì¶•ì œÂ·í–‰ì‚¬Â·ê³µì—°Â·ì‹œìƒì‹ ë“± ì¼íšŒì„± ì´ë²¤íŠ¸
   - ë‚ ì”¨, ê³„ì ˆì„± í’ê²½, ë‹¨ìˆœ í†µê³„Â·ì°¸ì„ì ìˆ˜Â·ìˆ˜ìƒ ì¸ì› ë“± ì •ë³´ë§Œ ë‚˜ì—´ëœ ê¸°ì‚¬
   - ë‹¨ìˆœ ì–¸ê¸‰ì´ë‚˜ í˜•ì‹ì  ë°œì–¸ë§Œ ìˆê³  ì •ì±…Â·ì‚¬íšŒì  ë§¥ë½ì´ ì—†ëŠ” ê¸°ì‚¬

ì¶œë ¥ í˜•ì‹(JSON):
{
  "off_topic": ["headline1", "headline2", ...],
  "reason": "ì™œ ìë¥¸ê±´ì§€ ì´ìœ ë¥¼ ì„¤ëª…"
}
"""

def review_sessions_with_llm(course):
    """LLMì´ ì½”ìŠ¤ ë‚´ ì„¸ì…˜ headlineì„ ê²€ìˆ˜í•˜ê³  off-topic ì„¸ì…˜ì„ ì œê±°"""
    headlines = "\n".join([f"- {s['headline']}" for s in course.get("sessions", [])])
    prompt = f"{PROMPT_SESSION_REVIEW}\n\nì½”ìŠ¤ëª…: {course['courseName']}\n\n{headlines}"

    resp = safe_request(prompt)
    if not resp:
        return course  # ì‹¤íŒ¨ ì‹œ ê·¸ëŒ€ë¡œ ë°˜í™˜

    # === ì‘ë‹µ ì „ì²˜ë¦¬ (```json ë“± ì œê±°) ===
    resp_clean = (
        resp.strip()
        .replace("```json", "")
        .replace("```", "")
        .strip()
    )

    try:
        result = json.loads(resp_clean)
        off_topics = set(result.get("off_topic", []))
    except Exception as e:
        print(f"[ì„¸ì…˜ ê²€ìˆ˜ JSON íŒŒì‹± ì˜¤ë¥˜] {e}\n{resp[:200]}...\n")
        return course

    # === ê´€ë ¨ ì—†ëŠ” ì„¸ì…˜ ì œê±° ===
    filtered_sessions = []
    removed_sessions = []
    for s in course["sessions"]:
        if s["headline"] in off_topics:
            removed_sessions.append(s["headline"])
        else:
            filtered_sessions.append(s)

    if removed_sessions:
        print(f"\n[{course['courseName']}] LLM í›„ì²˜ë¦¬ë¡œ ì œê±°ëœ ì„¸ì…˜ ({len(removed_sessions)}ê°œ):")
        for h in removed_sessions:
            print(f"  - {h}")

    course["sessions"] = filtered_sessions
    return course

# === ì¸ë±ìŠ¤ ì¬ì •ë ¬ í•¨ìˆ˜ ===
def reindex_courses(courses):
    """courseId ë° sessionId ì¬ì •ë ¬"""
    for i, course in enumerate(courses):
        course["courseId"] = i + 1  # 1ë¶€í„° ì‹œì‘
        for j, session in enumerate(course.get("sessions", [])):
            session["sessionId"] = j + 1
    return courses

# === ë©”ì¸ íŒŒì´í”„ë¼ì¸ ===
def main():
    for file in COURSE_DIR.glob("*.json"):
        print(f"\n{file.name} ì²˜ë¦¬ ì¤‘...")

        # === ë°ì´í„° ë¡œë“œ ===
        with open(file, "r", encoding="utf-8") as f:
            data = json.load(f)

        # === ğŸ”¸ì •ì¹˜ JSON íŒŒì¼ì€ í•„í„°ë§ ì—†ì´ ê·¸ëŒ€ë¡œ ë³µì‚¬ ===
        if "politics" in file.name.lower():
            out_path = FILTER_DIR / file.name
            with open(file, "r", encoding="utf-8") as src, open(out_path, "w", encoding="utf-8") as dst:
                dst.write(src.read())
            print(f"[ì •ì¹˜ íŒŒì¼] í•„í„°ë§ ê±´ë„ˆëœ€ â€” ì›ë³¸ ë³µì‚¬ ì™„ë£Œ â†’ {out_path.name}")
            continue  # ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°

        # === ë°ì´í„° ë¡œë“œ ===
        with open(file, "r", encoding="utf-8") as f:
            data = json.load(f)

        # === ì •ì¹˜ ì½”ìŠ¤ ì œì™¸ ===
        non_politics = [c for c in data if c.get("topic") != "ì •ì¹˜"]

        # === 1ï¸. LLM ì½”ìŠ¤ í•„í„°ë§ ===
        filter_results = filter_by_course_name(non_politics)
        result_map = {r["courseName"]: r for r in filter_results}

        print("\n=== LLM í•„í„°ë§ ê²°ê³¼ (ìš”ì•½) ===")
        print(json.dumps(filter_results, ensure_ascii=False, indent=2))

        # === 2ï¸. ë¶€ì ì ˆ ì½”ìŠ¤ ì œê±° ===
        filtered, removed = [], []
        for c in data:
            course_name = c["courseName"]
            result = result_map.get(course_name, {})
            if result.get("is_educational", True):
                filtered.append(c)
            else:
                removed.append({
                    "courseName": course_name,
                    "reason": result.get("reason", "ì´ìœ  ì—†ìŒ")
                })

        print(f"\ní•™ìŠµìš© {len(filtered)}ê°œ / ì œê±° {len(removed)}ê°œ")

        if removed:
            print("\n=== ì œê±°ëœ ì½”ìŠ¤ ëª©ë¡ ===")
            for r in removed:
                print(f"  - {r['courseName']}  ({r['reason']})")
            print("-" * 50)

        # === 3ï¸. ì½”ìŠ¤ ì¼ê´€ì„± + ì„¸ì…˜ ë…¸ì´ì¦ˆ ì œê±° ===
        coherence_scores = []
        print("\n=== ì½”ìŠ¤ë³„ ì„¸ì…˜ ë…¸ì´ì¦ˆ ì œê±° ë° ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚° ===")

        for c in tqdm(filtered, desc="2ë‹¨ê³„: ì„¸ì…˜ ì •ì œ ë° ì ìˆ˜ ê³„ì‚°"):
            score, removed_sessions = compute_coherence(c, noise_threshold=0.55, return_removed=True)
            coherence_scores.append({"courseName": c["courseName"], "score": score})

            if removed_sessions:
                print(f"\n[{c['courseName']}] ì œê±°ëœ ì„¸ì…˜ ({len(removed_sessions)}ê°œ):")
                for r in removed_sessions:
                    print(f"  - {r['headline']} ({r['similarity']:.3f})")

        coherence_scores.sort(key=lambda x: x["score"])

        # === 4. LLM ì„¸ì…˜ í›„ì²˜ë¦¬ ===  â† ì¶”ê°€
        print("\n=== 3ë‹¨ê³„: LLM ì„¸ì…˜ ì¼ê´€ì„± ì¬ê²€ìˆ˜ ===")
        refined = []
        for c in tqdm(filtered, desc="3ë‹¨ê³„: LLM ì„¸ì…˜ ê²€ìˆ˜"):
            reviewed = review_sessions_with_llm(c)
            if reviewed.get("sessions"):  # ì„¸ì…˜ì´ ë‚¨ì•„ìˆëŠ” ê²½ìš°ë§Œ ìœ ì§€
                refined.append(reviewed)
            else:
                print(f"{c['courseName']} ëª¨ë“  ì„¸ì…˜ì´ ì œê±°ë˜ì–´ ì œì™¸ë¨")
        filtered = refined

        # === 5. ì„¸ì…˜ ê°œìˆ˜ê°€ ë„ˆë¬´ ì ì€ ì½”ìŠ¤ ì œê±° === â˜… ì—¬ê¸°ì— ì¶”ê°€ â˜…
        final_courses = []
        removed_short = []
        for c in filtered:
            if len(c.get("sessions", [])) < 4:
                removed_short.append({
                    "courseName": c["courseName"],
                    "reason": f"ì„¸ì…˜ {len(c.get('sessions', []))}ê°œë¡œ, í•™ìŠµìš© ê¸°ì¤€(5ê°œ) ë¯¸ë‹¬"
                })
            else:
                final_courses.append(c)
        filtered = final_courses

        if removed_short:
            print(f"\n=== ì„¸ì…˜ ë¶€ì¡±ìœ¼ë¡œ ì œê±°ëœ ì½”ìŠ¤ ({len(removed_short)}ê°œ) ===")
            for r in removed_short:
                print(f"  - {r['courseName']} ({r['reason']})")
            print("-" * 50)

        # === 6. ì½”ìŠ¤ë³„ ì ìˆ˜ ì¶œë ¥ ===
        print("\n=== ì½”ìŠ¤ë³„ ì„¸ì…˜ ì¼ê´€ì„± ì ìˆ˜ ===")
        for s in coherence_scores:
            print(f"  - {s['courseName']}: {s['score']:.3f}")
        print("===============================")

        # === 7. ê°€ì¥ ì¼ê´€ì„± ë‚®ì€ ì½”ìŠ¤ í‘œì‹œ ===
        worst = coherence_scores[0] if coherence_scores else None
        if worst:
            print(f"\nì¼ê´€ì„± ê°€ì¥ ë‚®ì€ ì½”ìŠ¤: {worst['courseName']} ({worst['score']:.3f})")

        # === 8. ì¸ë±ìŠ¤ ì¬ì •ë ¬ ===
        reindexed_filtered = reindex_courses(filtered)

        # ë¶ˆí•„ìš”í•œ similarity í•„ë“œ ì œê±°ëŠ” reindexed_filtered ëŒ€ìƒìœ¼ë¡œ ìˆ˜í–‰
        for c in reindexed_filtered:
            for s in c.get("sessions", []):
                s.pop("similarity_to_center", None)

        # ì •ì¹˜ ì½”ìŠ¤(ì›ë³¸) + ì •ì œëœ ë¹„ì •ì¹˜ ì½”ìŠ¤ ë³‘í•©
        merged = reindexed_filtered

        # === 9. ê²°ê³¼ ì €ì¥ ===
        out_path = FILTER_DIR / f"{file.name}"
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(merged, f, ensure_ascii=False, indent=2)
        print(f"\ní•„í„°ë§ ë° ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ â†’ {out_path.name}")

    print("\nëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ.")

if __name__ == "__main__":
    print("=== ë‰´ìŠ¤ ì½”ìŠ¤ í•„í„°ë§ ì‹œì‘ ===")
    main()
    print("=== ëª¨ë“  ì½”ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ ===")